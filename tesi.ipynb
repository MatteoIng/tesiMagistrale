{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-26 11:27:48,237\tINFO tune.py:645 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=395206)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(QMix pid=395206)\u001b[0m 2023-09-26 11:27:53,467\tWARNING deprecation.py:50 -- DeprecationWarning: `rllib/algorithms/qmix/` has been deprecated. Use `rllib_contrib/qmix/` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(QMix pid=395206)\u001b[0m 2023-09-26 11:27:53,467\tWARNING deprecation.py:50 -- DeprecationWarning: `rllib/algorithms/simple_q/` has been deprecated. Use `rllib_contrib/simple_q/` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(QMix pid=395206)\u001b[0m 2023-09-26 11:27:53,467\tWARNING algorithm_config.py:672 -- Cannot create QMixConfig from given `config_dict`! Property __stdout_file__ not supported.\n",
      "\u001b[2m\u001b[36m(QMix pid=395206)\u001b[0m 2023-09-26 11:27:53,655\tWARNING multi_agent_env.py:274 -- observation_space_sample() of <GroupAgentsWrapper instance> has not been implemented. You can either implement it yourself or bring the observation space into the preferred format of a mapping from agent ids to their individual observation spaces. \n",
      "\u001b[2m\u001b[36m(QMix pid=395206)\u001b[0m 2023-09-26 11:27:53,656\tWARNING multi_agent_env.py:169 -- observation_space_contains() of <GroupAgentsWrapper instance> has not been implemented. You can either implement it yourself or bring the observation space into the preferred format of a mapping from agent ids to their individual observation spaces. \n",
      "\u001b[2m\u001b[36m(QMix pid=395206)\u001b[0m 2023-09-26 11:27:53,656\tWARNING multi_agent_env.py:169 -- observation_space_contains() of <GroupAgentsWrapper instance> has not been implemented. You can either implement it yourself or bring the observation space into the preferred format of a mapping from agent ids to their individual observation spaces. \n",
      "\u001b[2m\u001b[36m(QMix pid=395206)\u001b[0m 2023-09-26 11:27:53,656\tWARNING multi_agent_env.py:169 -- observation_space_contains() of <GroupAgentsWrapper instance> has not been implemented. You can either implement it yourself or bring the observation space into the preferred format of a mapping from agent ids to their individual observation spaces. \n",
      "\u001b[2m\u001b[36m(QMix pid=395206)\u001b[0m 2023-09-26 11:27:53,656\tWARNING multi_agent_env.py:237 -- action_space_sample() of <GroupAgentsWrapper instance> has not been implemented. You can either implement it yourself or bring the observation space into the preferred format of a mapping from agent ids to their individual observation spaces.\n",
      "\u001b[2m\u001b[36m(QMix pid=395206)\u001b[0m 2023-09-26 11:27:53,656\tWARNING multi_agent_env.py:199 -- action_space_contains() of <GroupAgentsWrapper instance> has not been implemented. You can either implement it yourself or bring the observation space into the preferred format of a mapping from agent ids to their individual observation spaces. \n",
      "\u001b[2m\u001b[36m(QMix pid=395206)\u001b[0m 2023-09-26 11:27:53,656\tWARNING multi_agent_env.py:169 -- observation_space_contains() of <GroupAgentsWrapper instance> has not been implemented. You can either implement it yourself or bring the observation space into the preferred format of a mapping from agent ids to their individual observation spaces. \n",
      "2023-09-26 11:27:53,805\tERROR tune_controller.py:1502 -- Trial task failed for trial QMIX_grouped_test_f4fbd_00000\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/matteo/.local/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/home/matteo/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/matteo/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/matteo/.local/lib/python3.10/site-packages/ray/_private/worker.py\", line 2549, in get\n",
      "    raise value\n",
      "ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::QMix.__init__()\u001b[39m (pid=395206, ip=10.25.252.249, actor_id=9bcc438ad072c0cc9fd7b53a01000000, repr=QMix)\n",
      "  File \"/home/matteo/.local/lib/python3.10/site-packages/ray/rllib/utils/deprecation.py\", line 106, in patched_init\n",
      "    return obj_init(*args, **kwargs)\n",
      "  File \"/home/matteo/.local/lib/python3.10/site-packages/ray/rllib/utils/deprecation.py\", line 106, in patched_init\n",
      "    return obj_init(*args, **kwargs)\n",
      "  File \"/home/matteo/.local/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 517, in __init__\n",
      "    super().__init__(\n",
      "  File \"/home/matteo/.local/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 185, in __init__\n",
      "    self.setup(copy.deepcopy(self.config))\n",
      "  File \"/home/matteo/.local/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 639, in setup\n",
      "    self.workers = WorkerSet(\n",
      "  File \"/home/matteo/.local/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py\", line 157, in __init__\n",
      "    self._setup(\n",
      "  File \"/home/matteo/.local/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py\", line 247, in _setup\n",
      "    self._local_worker = self._make_worker(\n",
      "  File \"/home/matteo/.local/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py\", line 925, in _make_worker\n",
      "    worker = cls(\n",
      "  File \"/home/matteo/.local/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 470, in __init__\n",
      "    self.policy_dict, self.is_policy_to_train = self.config.get_multi_agent_setup(\n",
      "  File \"/home/matteo/.local/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm_config.py\", line 2973, in get_multi_agent_setup\n",
      "    raise ValueError(\n",
      "ValueError: `observation_space` not provided in PolicySpec for default_policy and env does not have an observation space OR no spaces received from other workers' env(s) OR no `observation_space` specified in config!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(QMix pid=395206)\u001b[0m Agente in azione: attaccante\n",
      "\u001b[2m\u001b[36m(QMix pid=395206)\u001b[0m Agente in azione: difensore\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>QMIX_grouped_test_f4fbd_00000</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TuneError",
     "evalue": "('Trials did not complete', [QMIX_grouped_test_f4fbd_00000])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTuneError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_386721/2454478784.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0mregister_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"grouped_test\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_creator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m results = tune.run(\n\u001b[0m\u001b[1;32m    116\u001b[0m     \u001b[0;34m\"QMIX\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"timesteps_total\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/ray/tune/tune.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, storage_path, storage_filesystem, search_alg, scheduler, checkpoint_config, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, sync_config, export_formats, max_failures, fail_fast, restore, server_port, resume, reuse_actors, raise_on_failed_trial, callbacks, max_concurrent_trials, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, chdir_to_trial_dir, local_dir, _experiment_checkpoint_dir, _remote, _remote_string_queue, _entrypoint)\u001b[0m\n\u001b[1;32m   1135\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mincomplete_trials\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mraise_on_failed_trial\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mexperiment_interrupted_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1137\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTuneError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trials did not complete\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincomplete_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1138\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trials did not complete: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincomplete_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTuneError\u001b[0m: ('Trials did not complete', [QMIX_grouped_test_f4fbd_00000])"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(QMix pid=395206)\u001b[0m Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::QMix.__init__()\u001b[39m (pid=395206, ip=10.25.252.249, actor_id=9bcc438ad072c0cc9fd7b53a01000000, repr=QMix)\n",
      "\u001b[2m\u001b[36m(QMix pid=395206)\u001b[0m   File \"/home/matteo/.local/lib/python3.10/site-packages/ray/rllib/utils/deprecation.py\", line 106, in patched_init\n",
      "\u001b[2m\u001b[36m(QMix pid=395206)\u001b[0m     return obj_init(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(QMix pid=395206)\u001b[0m   File \"/home/matteo/.local/lib/python3.10/site-packages/ray/rllib/utils/deprecation.py\", line 106, in patched_init\n",
      "\u001b[2m\u001b[36m(QMix pid=395206)\u001b[0m     return obj_init(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(QMix pid=395206)\u001b[0m   File \"/home/matteo/.local/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 517, in __init__\n",
      "\u001b[2m\u001b[36m(QMix pid=395206)\u001b[0m     super().__init__(\n",
      "\u001b[2m\u001b[36m(QMix pid=395206)\u001b[0m   File \"/home/matteo/.local/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 185, in __init__\n",
      "\u001b[2m\u001b[36m(QMix pid=395206)\u001b[0m     self.setup(copy.deepcopy(self.config))\n",
      "\u001b[2m\u001b[36m(QMix pid=395206)\u001b[0m   File \"/home/matteo/.local/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 639, in setup\n",
      "\u001b[2m\u001b[36m(QMix pid=395206)\u001b[0m     self.workers = WorkerSet(\n",
      "\u001b[2m\u001b[36m(QMix pid=395206)\u001b[0m   File \"/home/matteo/.local/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py\", line 157, in __init__\n",
      "\u001b[2m\u001b[36m(QMix pid=395206)\u001b[0m     self._setup(\n",
      "\u001b[2m\u001b[36m(QMix pid=395206)\u001b[0m   File \"/home/matteo/.local/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py\", line 247, in _setup\n",
      "\u001b[2m\u001b[36m(QMix pid=395206)\u001b[0m     self._local_worker = self._make_worker(\n",
      "\u001b[2m\u001b[36m(QMix pid=395206)\u001b[0m   File \"/home/matteo/.local/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py\", line 925, in _make_worker\n",
      "\u001b[2m\u001b[36m(QMix pid=395206)\u001b[0m     worker = cls(\n",
      "\u001b[2m\u001b[36m(QMix pid=395206)\u001b[0m   File \"/home/matteo/.local/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 470, in __init__\n",
      "\u001b[2m\u001b[36m(QMix pid=395206)\u001b[0m     self.policy_dict, self.is_policy_to_train = self.config.get_multi_agent_setup(\n",
      "\u001b[2m\u001b[36m(QMix pid=395206)\u001b[0m   File \"/home/matteo/.local/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm_config.py\", line 2973, in get_multi_agent_setup\n",
      "\u001b[2m\u001b[36m(QMix pid=395206)\u001b[0m     raise ValueError(\n",
      "\u001b[2m\u001b[36m(QMix pid=395206)\u001b[0m ValueError: `observation_space` not provided in PolicySpec for default_policy and env does not have an observation space OR no spaces received from other workers' env(s) OR no `observation_space` specified in config!\n"
     ]
    }
   ],
   "source": [
    "from gym.spaces import Tuple, Discrete, Box,Dict\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.registry import register_env\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "import numpy as np\n",
    "from ray.rllib.utils import check_env\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "class TestEnv(MultiAgentEnv):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # Mosse \n",
    "        self.MOVES = ['mossa1', 'mossa2','mossa3']\n",
    "        # Questa è la truncation cosi esce per non girare all'infinito\n",
    "        self.NUM_ITERS = 20\n",
    "        # Mappa che in base all'azione eseguita mi da costo, impatto, ecc dell'azione\n",
    "        self.REWARD_MAP = {\n",
    "            'mossa1': (1, 0, 0),\n",
    "            'mossa2': (2, 0, 0),\n",
    "            'mossa3': (3, 0, 0)\n",
    "        }\n",
    "        # per la funzione di reward\n",
    "        self.wt = 0.5\n",
    "        self.wc = 0.5\n",
    "        self.wi = 0.5\n",
    "        self.tMax = 100\n",
    "        self.cMax = 100\n",
    "        self.possible_agents = ['attaccante','difensore']\n",
    "        self._agent_ids = set(self.possible_agents)\n",
    "        self._action_spaces = {agent: Discrete(3) for agent in self.possible_agents}\n",
    "        self._observation_spaces = {\n",
    "            agent:Dict(\n",
    "                {\n",
    "                    'observation': Box(low=0,high=1,shape=(2,),dtype=bool),\n",
    "                }\n",
    "            )\n",
    "            for agent in self.possible_agents\n",
    "        }\n",
    "\n",
    "    def step(self,allAction):\n",
    "        \"\"\" if (\n",
    "            self.terminations[self.agent_selection]\n",
    "            or self.truncations[self.agent_selection]\n",
    "        ):\n",
    "            #self._was_dead_step(action)\n",
    "            print('ECCOOOO:',self.agents)\n",
    "            return \"\"\"\n",
    "\n",
    "        for agent in self.agents:\n",
    "            #agent = self.agent_selection\n",
    "            print('Agente in azione:',agent)\n",
    "            \"\"\" action = allAction[agent]\n",
    "            reward = self.REWARD_MAP[self.MOVES[action]]\n",
    "            valReward = -self.wt*(reward[0]/self.tMax)-self.wc*(reward[1]/self.cMax)-self.wi*(1) \n",
    "            #rewardInv = -valReward\n",
    "            print('Reward:',valReward)\n",
    "            if agent == 'attaccante':\n",
    "                self.rewards[self.agents[0]], self.rewards[self.agents[1]] = (valReward,0)\n",
    "            else:\n",
    "                self.rewards[self.agents[0]], self.rewards[self.agents[1]] = (0,valReward) \"\"\"\n",
    "            self.num_moves += 1\n",
    "            # The truncations dictionary must be updated for all players.\n",
    "            self.truncations = {\n",
    "                agent: self.num_moves >= self.NUM_ITERS for agent in self.agents\n",
    "            }\n",
    "            #self.agent_selection = self._agent_selector.next()\n",
    "            # Adds .rewards to ._cumulative_rewards\n",
    "            #self._accumulate_rewards() \n",
    "        return self._observation_spaces,self.rewards,self.terminations,self.truncations,self.infos\n",
    "        \n",
    "\n",
    "        \n",
    "    def reset(self,seed=None,options=None):\n",
    "        super().reset()\n",
    "        self.agents = self.possible_agents[:]\n",
    "        self.rewards = {agent: 0 for agent in self.agents}\n",
    "        self._cumulative_rewards = {agent: 0 for agent in self.agents}      \n",
    "        self.terminations = {agent: False for agent in self.agents}\n",
    "        self.truncations = {agent: False for agent in self.agents}\n",
    "        self.infos = {agent: {} for agent in self.agents}\n",
    "        #self.state = {agent: NONE for agent in self.agents}\n",
    "        #self.observations = {agent: NONE for agent in self.agents}\n",
    "        # credo serve per arrestare\n",
    "        self.num_moves = 0\n",
    "        \n",
    "        #Our agent_selector utility allows easy cyclic stepping through the agents list.\n",
    "        \n",
    "        #self._agent_selector = agent_selector(self.agents)\n",
    "        #self.agent_selection = self._agent_selector.reset()\n",
    "        return self._observation_spaces,self.infos\n",
    "    \n",
    "    def observation_space(self, agent):\n",
    "        # gymnasium spaces are defined and documented here: https://gymnasium.farama.org/api/spaces/\n",
    "        return self._observation_spaces[agent]\n",
    "    \n",
    "    def action_space(self, agent):\n",
    "        return self._action_spaces[agent]\n",
    "        \n",
    "\n",
    "\n",
    "def env_creator(args):\n",
    "    env = TestEnv(args)\n",
    "    obs_space = env.observation_space\n",
    "    act_space = env.action_space\n",
    "    n_agents = len(args[\"agents\"])\n",
    "    obs_space = ([obs_space for _ in range(n_agents)])\n",
    "    act_space = ([act_space for _ in range(n_agents)])\n",
    "    grouping = {'group1':env.possible_agents}\n",
    "    return env.with_agent_groups( grouping,obs_space=obs_space, act_space=act_space)\n",
    "\n",
    "\n",
    "register_env(\"grouped_test\", env_creator)\n",
    "results = tune.run(\n",
    "    \"QMIX\",\n",
    "    stop={\"timesteps_total\": 5,},\n",
    "    config={\n",
    "        \"mixer\": \"qmix\",\n",
    "        \"env\": \"grouped_test\",\n",
    "        \"env_config\": {\"agents\": [\"attaccante\", \"difensore\"]},\n",
    "        \"num_workers\": 0,\n",
    "        \"timesteps_per_iteration\": 2,\n",
    "    },\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
