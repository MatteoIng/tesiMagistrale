{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.examples.policy.rock_paper_scissors_dummies import (\n",
    "    BeatLastHeuristic,\n",
    "    AlwaysSameHeuristic,\n",
    ")\n",
    "import os\n",
    "import random\n",
    "from ray.rllib.policy.policy import PolicySpec\n",
    "from ray.rllib.algorithms.algorithm_config import AlgorithmConfig\n",
    "from gym.spaces import Tuple, Discrete, Box,Dict\n",
    "import ray\n",
    "from ray import air\n",
    "from ray import tune\n",
    "from ray.tune.registry import register_env\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "import numpy as np\n",
    "from ray.rllib.utils import check_env\n",
    "from typing import Optional\n",
    "from ray.rllib.algorithms.pg import (\n",
    "    PG,\n",
    "    PGConfig,\n",
    "    PGTF2Policy,\n",
    "    PGTF1Policy,\n",
    "    PGTorchPolicy,\n",
    ")\n",
    "import argparse\n",
    "from ray.rllib.utils.test_utils import check_learning_achieved\n",
    "from ray.rllib.utils.framework import try_import_tf, try_import_torch\n",
    "from ray.rllib.algorithms.maddpg.maddpg import MADDPGConfig,DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-26 17:24:29,948\tINFO worker.py:1633 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    }
   ],
   "source": [
    "class TestEnv(MultiAgentEnv):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # Mosse \n",
    "        self.MOVES = ['mossa1', 'mossa2','mossa3']\n",
    "        # Questa Ã¨ la truncation cosi esce per non girare all'infinito\n",
    "        self.NUM_ITERS = 20\n",
    "        # Mappa che in base all'azione eseguita mi da costo, impatto, ecc dell'azione\n",
    "        self.REWARD_MAP = {\n",
    "            'mossa1': (1, 0, 0),\n",
    "            'mossa2': (2, 0, 0),\n",
    "            'mossa3': (3, 0, 0)\n",
    "        }\n",
    "        # per la funzione di reward\n",
    "        self.wt = 0.5\n",
    "        self.wc = 0.5\n",
    "        self.wi = 0.5\n",
    "        self.tMax = 100\n",
    "        self.cMax = 100\n",
    "        self.possible_agents = ['attaccante','difensore']\n",
    "        self._agent_ids = set(self.possible_agents)\n",
    "        self._action_spaces = {agent: Discrete(3) for agent in self.possible_agents}\n",
    "        self._observation_spaces = {\n",
    "            agent:Dict(\n",
    "                {\n",
    "                    'observation': Box(low=0,high=1,shape=(2,),dtype=np.float32),\n",
    "                }\n",
    "            )\n",
    "            for agent in self.possible_agents\n",
    "        }\n",
    "        \n",
    "\n",
    "    def step(self,allAction):\n",
    "        \"\"\" if (\n",
    "            self.terminations[self.agent_selection]\n",
    "            or self.truncations[self.agent_selection]\n",
    "        ):\n",
    "            #self._was_dead_step(action)\n",
    "            print('ECCOOOO:',self.agents)\n",
    "            return \"\"\"\n",
    "\n",
    "        for agent in self.agents:\n",
    "            #agent = self.agent_selection\n",
    "            print('Agente in azione:',agent)\n",
    "            \"\"\" action = allAction[agent]\n",
    "            reward = self.REWARD_MAP[self.MOVES[action]]\n",
    "            valReward = -self.wt*(reward[0]/self.tMax)-self.wc*(reward[1]/self.cMax)-self.wi*(1) \n",
    "            #rewardInv = -valReward\n",
    "            print('Reward:',valReward)\n",
    "            if agent == 'attaccante':\n",
    "                self.rewards[self.agents[0]], self.rewards[self.agents[1]] = (valReward,0)\n",
    "            else:\n",
    "                self.rewards[self.agents[0]], self.rewards[self.agents[1]] = (0,valReward) \"\"\"\n",
    "            self.num_moves += 1\n",
    "            # The truncations dictionary must be updated for all players.\n",
    "            self.truncations = {\n",
    "                agent: self.num_moves >= self.NUM_ITERS for agent in self.agents\n",
    "            }\n",
    "            #self.agent_selection = self._agent_selector.next()\n",
    "            # Adds .rewards to ._cumulative_rewards\n",
    "            #self._accumulate_rewards() \n",
    "        return self._observation_spaces,self.rewards,self.terminations,self.truncations,self.infos\n",
    "        \n",
    "\n",
    "        \n",
    "    def reset(self,seed=None,options=None):\n",
    "        super().reset()\n",
    "        self.agents = self.possible_agents[:]\n",
    "        self.rewards = {agent: 0 for agent in self.agents}\n",
    "        self._cumulative_rewards = {agent: 0 for agent in self.agents}      \n",
    "        self.terminations = {agent: False for agent in self.agents}\n",
    "        self.truncations = {agent: False for agent in self.agents}\n",
    "        self.infos = {agent: {} for agent in self.agents}\n",
    "        #self.state = {agent: NONE for agent in self.agents}\n",
    "        #self.observations = {agent: NONE for agent in self.agents}\n",
    "        # credo serve per arrestare\n",
    "        self.num_moves = 0\n",
    "        \n",
    "        #Our agent_selector utility allows easy cyclic stepping through the agents list.\n",
    "        \n",
    "        #self._agent_selector = agent_selector(self.agents)\n",
    "        #self.agent_selection = self._agent_selector.reset()\n",
    "        return self._observation_spaces,self.infos\n",
    "    \n",
    "    def observation_space(self, agent):\n",
    "        # gymnasium spaces are defined and documented here: https://gymnasium.farama.org/api/spaces/\n",
    "        return self._observation_spaces[agent]\n",
    "    \n",
    "    def action_space(self, agent):\n",
    "        return self._action_spaces[agent]\n",
    "        \n",
    "\n",
    "\n",
    "def env_creator(args):\n",
    "    env = TestEnv(args)\n",
    "\n",
    "    obs_space = {agent: env.observation_space(agent)['observation'] for agent in env.possible_agents}\n",
    "    act_space = {agent: env.action_space(agent) for agent in env.possible_agents}\n",
    "    print('obs_space:',obs_space)\n",
    "    print('act_space:',act_space)\n",
    "\n",
    "    #n_agents = len(args[\"agents\"])\n",
    "    grouping = {'group1':env.possible_agents}\n",
    "    return env.with_agent_groups( grouping,obs_space=obs_space, act_space=act_space)\n",
    "\n",
    "ray.shutdown()\n",
    "ray.init(num_gpus=1)\n",
    "\n",
    "register_env(\"grouped_test\", env_creator)\n",
    "\"\"\"results = tune.run(\n",
    "    \"QMIX\",\n",
    "    stop={\"timesteps_total\": 5,},\n",
    "    config={\n",
    "        \"mixer\": \"qmix\",\n",
    "        \"env\": \"grouped_test\",\n",
    "        \"observation_space\": {\n",
    "        \"attaccante\": Box(low=0,high=1,shape=(2,),dtype=np.float32),\n",
    "        \"difensore\": Box(low=0,high=1,shape=(2,),dtype=np.float32),\n",
    "    },\n",
    "        \"env_config\": {\"agents\": [\"attaccante\", \"difensore\"]},\n",
    "        \"num_workers\": 0,\n",
    "        \"timesteps_per_iteration\": 2,\n",
    "        \"observetion_space\":None,\n",
    "    },\n",
    ") \"\"\"\n",
    "\n",
    "tf1, tf, tfv = try_import_tf()\n",
    "torch, _ = try_import_torch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=461673)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=461673)\u001b[0m 2023-09-26 17:25:35,895\tWARNING deprecation.py:50 -- DeprecationWarning: `build_tf_policy` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=461673)\u001b[0m 2023-09-26 17:25:35,900\tWARNING deprecation.py:50 -- DeprecationWarning: `build_policy_class` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(MADDPG pid=461673)\u001b[0m 2023-09-26 17:25:35,972\tWARNING deprecation.py:50 -- DeprecationWarning: `rllib/algorithms/maddpg/` has been deprecated. Use `rllib_contrib/maddpg/` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(MADDPG pid=461673)\u001b[0m 2023-09-26 17:25:35,972\tWARNING deprecation.py:50 -- DeprecationWarning: `rllib/algorithms/simple_q/` has been deprecated. Use `rllib_contrib/simple_q/` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(MADDPG pid=461673)\u001b[0m 2023-09-26 17:25:35,973\tWARNING algorithm_config.py:672 -- Cannot create MADDPGConfig from given `config_dict`! Property __stdout_file__ not supported.\n",
      "\u001b[2m\u001b[36m(pid=461723)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(MADDPG pid=461673)\u001b[0m 2023-09-26 17:25:47,573\tERROR actor_manager.py:500 -- Ray error, taking actor 1 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=461723, ip=10.25.252.249, actor_id=e86d2fdb7d440ca01819126701000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fad519af940>)\n",
      "\u001b[2m\u001b[36m(MADDPG pid=461673)\u001b[0m   File \"/home/matteo/.local/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 470, in __init__\n",
      "\u001b[2m\u001b[36m(MADDPG pid=461673)\u001b[0m     self.policy_dict, self.is_policy_to_train = self.config.get_multi_agent_setup(\n",
      "\u001b[2m\u001b[36m(MADDPG pid=461673)\u001b[0m   File \"/home/matteo/.local/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm_config.py\", line 2973, in get_multi_agent_setup\n",
      "\u001b[2m\u001b[36m(MADDPG pid=461673)\u001b[0m     raise ValueError(\n",
      "\u001b[2m\u001b[36m(MADDPG pid=461673)\u001b[0m ValueError: `observation_space` not provided in PolicySpec for default_policy and env does not have an observation space OR no spaces received from other workers' env(s) OR no `observation_space` specified in config!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=461723)\u001b[0m 2023-09-26 17:25:47,539\tWARNING deprecation.py:50 -- DeprecationWarning: `build_tf_policy` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=461723)\u001b[0m 2023-09-26 17:25:47,543\tWARNING deprecation.py:50 -- DeprecationWarning: `build_policy_class` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=461723)\u001b[0m 2023-09-26 17:25:47,550\tWARNING multi_agent_env.py:274 -- observation_space_sample() of <GroupAgentsWrapper instance> has not been implemented. You can either implement it yourself or bring the observation space into the preferred format of a mapping from agent ids to their individual observation spaces. \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=461723)\u001b[0m 2023-09-26 17:25:47,550\tWARNING multi_agent_env.py:169 -- observation_space_contains() of <GroupAgentsWrapper instance> has not been implemented. You can either implement it yourself or bring the observation space into the preferred format of a mapping from agent ids to their individual observation spaces. \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=461723)\u001b[0m 2023-09-26 17:25:47,551\tWARNING multi_agent_env.py:169 -- observation_space_contains() of <GroupAgentsWrapper instance> has not been implemented. You can either implement it yourself or bring the observation space into the preferred format of a mapping from agent ids to their individual observation spaces. \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=461723)\u001b[0m 2023-09-26 17:25:47,551\tWARNING multi_agent_env.py:169 -- observation_space_contains() of <GroupAgentsWrapper instance> has not been implemented. You can either implement it yourself or bring the observation space into the preferred format of a mapping from agent ids to their individual observation spaces. \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=461723)\u001b[0m 2023-09-26 17:25:47,551\tWARNING multi_agent_env.py:237 -- action_space_sample() of <GroupAgentsWrapper instance> has not been implemented. You can either implement it yourself or bring the observation space into the preferred format of a mapping from agent ids to their individual observation spaces.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=461723)\u001b[0m 2023-09-26 17:25:47,551\tWARNING multi_agent_env.py:199 -- action_space_contains() of <GroupAgentsWrapper instance> has not been implemented. You can either implement it yourself or bring the observation space into the preferred format of a mapping from agent ids to their individual observation spaces. \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=461723)\u001b[0m 2023-09-26 17:25:47,551\tWARNING multi_agent_env.py:169 -- observation_space_contains() of <GroupAgentsWrapper instance> has not been implemented. You can either implement it yourself or bring the observation space into the preferred format of a mapping from agent ids to their individual observation spaces. \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=461723)\u001b[0m Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=461723, ip=10.25.252.249, actor_id=e86d2fdb7d440ca01819126701000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fad519af940>)\n",
      "\u001b[2m\u001b[36m(MADDPG pid=461673)\u001b[0m Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::MADDPG.__init__()\u001b[39m (pid=461673, ip=10.25.252.249, actor_id=afec67bba5aba35c4110870801000000, repr=MADDPG)\n",
      "\u001b[2m\u001b[36m(MADDPG pid=461673)\u001b[0m   File \"/home/matteo/.local/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py\", line 227, in _setup\n",
      "\u001b[2m\u001b[36m(MADDPG pid=461673)\u001b[0m     self.add_workers(\n",
      "\u001b[2m\u001b[36m(MADDPG pid=461673)\u001b[0m   File \"/home/matteo/.local/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py\", line 593, in add_workers\n",
      "\u001b[2m\u001b[36m(MADDPG pid=461673)\u001b[0m     raise result.get()\n",
      "\u001b[2m\u001b[36m(MADDPG pid=461673)\u001b[0m   File \"/home/matteo/.local/lib/python3.10/site-packages/ray/rllib/utils/actor_manager.py\", line 481, in __fetch_result\n",
      "\u001b[2m\u001b[36m(MADDPG pid=461673)\u001b[0m     result = ray.get(r)\n",
      "\u001b[2m\u001b[36m(MADDPG pid=461673)\u001b[0m ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=461723, ip=10.25.252.249, actor_id=e86d2fdb7d440ca01819126701000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fad519af940>)\n",
      "\u001b[2m\u001b[36m(MADDPG pid=461673)\u001b[0m \n",
      "\u001b[2m\u001b[36m(MADDPG pid=461673)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(MADDPG pid=461673)\u001b[0m \n",
      "\u001b[2m\u001b[36m(MADDPG pid=461673)\u001b[0m \u001b[36mray::MADDPG.__init__()\u001b[39m (pid=461673, ip=10.25.252.249, actor_id=afec67bba5aba35c4110870801000000, repr=MADDPG)\n",
      "\u001b[2m\u001b[36m(MADDPG pid=461673)\u001b[0m   File \"/home/matteo/.local/lib/python3.10/site-packages/ray/rllib/utils/deprecation.py\", line 106, in patched_init\n",
      "\u001b[2m\u001b[36m(MADDPG pid=461673)\u001b[0m     return obj_init(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(MADDPG pid=461673)\u001b[0m   File \"/home/matteo/.local/lib/python3.10/site-packages/ray/rllib/utils/deprecation.py\", line 106, in patched_init\n",
      "\u001b[2m\u001b[36m(MADDPG pid=461673)\u001b[0m     return obj_init(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(MADDPG pid=461673)\u001b[0m     super().__init__(\n",
      "\u001b[2m\u001b[36m(MADDPG pid=461673)\u001b[0m     self.setup(copy.deepcopy(self.config))\n",
      "\u001b[2m\u001b[36m(MADDPG pid=461673)\u001b[0m   File \"/home/matteo/.local/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 639, in setup\n",
      "\u001b[2m\u001b[36m(MADDPG pid=461673)\u001b[0m     self.workers = WorkerSet(\n",
      "\u001b[2m\u001b[36m(MADDPG pid=461673)\u001b[0m     raise e.args[0].args[2]\n",
      "2023-09-26 17:25:47,603\tERROR tune_controller.py:1502 -- Trial task failed for trial MADDPG_grouped_test_e9b2a_00000\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/matteo/.local/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/home/matteo/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/matteo/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/matteo/.local/lib/python3.10/site-packages/ray/_private/worker.py\", line 2549, in get\n",
      "    raise value\n",
      "ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::MADDPG.__init__()\u001b[39m (pid=461673, ip=10.25.252.249, actor_id=afec67bba5aba35c4110870801000000, repr=MADDPG)\n",
      "  File \"/home/matteo/.local/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py\", line 227, in _setup\n",
      "    self.add_workers(\n",
      "  File \"/home/matteo/.local/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py\", line 593, in add_workers\n",
      "    raise result.get()\n",
      "  File \"/home/matteo/.local/lib/python3.10/site-packages/ray/rllib/utils/actor_manager.py\", line 481, in __fetch_result\n",
      "    result = ray.get(r)\n",
      "ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=461723, ip=10.25.252.249, actor_id=e86d2fdb7d440ca01819126701000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fad519af940>)\n",
      "  File \"/home/matteo/.local/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 470, in __init__\n",
      "    self.policy_dict, self.is_policy_to_train = self.config.get_multi_agent_setup(\n",
      "  File \"/home/matteo/.local/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm_config.py\", line 2973, in get_multi_agent_setup\n",
      "    raise ValueError(\n",
      "ValueError: `observation_space` not provided in PolicySpec for default_policy and env does not have an observation space OR no spaces received from other workers' env(s) OR no `observation_space` specified in config!\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::MADDPG.__init__()\u001b[39m (pid=461673, ip=10.25.252.249, actor_id=afec67bba5aba35c4110870801000000, repr=MADDPG)\n",
      "  File \"/home/matteo/.local/lib/python3.10/site-packages/ray/rllib/utils/deprecation.py\", line 106, in patched_init\n",
      "    return obj_init(*args, **kwargs)\n",
      "  File \"/home/matteo/.local/lib/python3.10/site-packages/ray/rllib/utils/deprecation.py\", line 106, in patched_init\n",
      "    return obj_init(*args, **kwargs)\n",
      "  File \"/home/matteo/.local/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 517, in __init__\n",
      "    super().__init__(\n",
      "  File \"/home/matteo/.local/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 185, in __init__\n",
      "    self.setup(copy.deepcopy(self.config))\n",
      "  File \"/home/matteo/.local/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 639, in setup\n",
      "    self.workers = WorkerSet(\n",
      "  File \"/home/matteo/.local/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py\", line 179, in __init__\n",
      "    raise e.args[0].args[2]\n",
      "ValueError: `observation_space` not provided in PolicySpec for default_policy and env does not have an observation space OR no spaces received from other workers' env(s) OR no `observation_space` specified in config!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=461723)\u001b[0m obs_space: {'attaccante': Box(0.0, 1.0, (2,), float32), 'difensore': Box(0.0, 1.0, (2,), float32)}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=461723)\u001b[0m act_space: {'attaccante': Discrete(3), 'difensore': Discrete(3)}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=461723)\u001b[0m Agente in azione: attaccante\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=461723)\u001b[0m Agente in azione: difensore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-26 17:25:47,615\tERROR tune.py:1139 -- Trials did not complete: [MADDPG_grouped_test_e9b2a_00000]\n",
      "2023-09-26 17:25:47,616\tINFO tune.py:1143 -- Total run time: 23.48 seconds (23.45 seconds for the tuning loop).\n",
      "2023-09-26 17:25:47,619\tWARNING experiment_analysis.py:205 -- Failed to fetch metrics for 1 trial(s):\n",
      "- MADDPG_grouped_test_e9b2a_00000: FileNotFoundError('Could not fetch metrics for MADDPG_grouped_test_e9b2a_00000: both result.json and progress.csv were not found at /home/matteo/ray_results/MADDPG_2023-09-26_17-25-24/MADDPG_grouped_test_e9b2a_00000_0_2023-09-26_17-25-24')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResultGrid<[\n",
      "  Result(\n",
      "    error='TuneError',\n",
      "    metrics={},\n",
      "    path='/home/matteo/ray_results/MADDPG_2023-09-26_17-25-24/MADDPG_grouped_test_e9b2a_00000_0_2023-09-26_17-25-24',\n",
      "    filesystem='local',\n",
      "    checkpoint=None\n",
      "  )\n",
      "]>\n"
     ]
    }
   ],
   "source": [
    "config = MADDPGConfig().environment(\"grouped_test\").framework(\"torch\").resources(num_gpus=1)\n",
    "#config = PGConfig().environment(\"grouped_test\").framework(\"torch\").resources(num_gpus=1)\n",
    "stop = {\n",
    "        \"training_iteration\": 150,\n",
    "        \"timesteps_total\": 100000,\n",
    "        \"episode_reward_mean\": 1000.0,\n",
    "    }\n",
    "\"\"\" results = tune.Tuner(\n",
    "        \"PG\", param_space=config, run_config=air.RunConfig(stop=stop, verbose=1)\n",
    "    ).fit() \"\"\"\n",
    "results = tune.Tuner(\n",
    "        \"MADDPG\", param_space=config, run_config=air.RunConfig(stop=stop, verbose=1)\n",
    "    ).fit()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matteo/.local/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py:484: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "/home/matteo/.local/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/home/matteo/.local/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/home/matteo/.local/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "2023-09-26 17:26:15,957\tWARNING multi_agent_env.py:169 -- observation_space_contains() of <GroupAgentsWrapper instance> has not been implemented. You can either implement it yourself or bring the observation space into the preferred format of a mapping from agent ids to their individual observation spaces. \n",
      "2023-09-26 17:26:15,959\tWARNING multi_agent_env.py:169 -- observation_space_contains() of <GroupAgentsWrapper instance> has not been implemented. You can either implement it yourself or bring the observation space into the preferred format of a mapping from agent ids to their individual observation spaces. \n",
      "2023-09-26 17:26:15,960\tWARNING multi_agent_env.py:169 -- observation_space_contains() of <GroupAgentsWrapper instance> has not been implemented. You can either implement it yourself or bring the observation space into the preferred format of a mapping from agent ids to their individual observation spaces. \n",
      "2023-09-26 17:26:15,962\tWARNING multi_agent_env.py:237 -- action_space_sample() of <GroupAgentsWrapper instance> has not been implemented. You can either implement it yourself or bring the observation space into the preferred format of a mapping from agent ids to their individual observation spaces.\n",
      "2023-09-26 17:26:15,963\tWARNING multi_agent_env.py:169 -- observation_space_contains() of <GroupAgentsWrapper instance> has not been implemented. You can either implement it yourself or bring the observation space into the preferred format of a mapping from agent ids to their individual observation spaces. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs_space: {'attaccante': Box(0.0, 1.0, (2,), float32), 'difensore': Box(0.0, 1.0, (2,), float32)}\n",
      "act_space: {'attaccante': Discrete(3), 'difensore': Discrete(3)}\n",
      "Agente in azione: attaccante\n",
      "Agente in azione: difensore\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "`observation_space` not provided in PolicySpec for always_same and env does not have an observation space OR no spaces received from other workers' env(s) OR no `observation_space` specified in config!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_404125/179083927.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0mresources\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_gpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RLLIB_NUM_GPUS\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     )\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0malgo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm_config.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, env, logger_creator, use_copy)\u001b[0m\n\u001b[1;32m   1106\u001b[0m             \u001b[0malgo_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_trainable_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgo_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1108\u001b[0;31m         return algo_class(\n\u001b[0m\u001b[1;32m   1109\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0muse_copy\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1110\u001b[0m             \u001b[0mlogger_creator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger_creator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/ray/rllib/utils/deprecation.py\u001b[0m in \u001b[0;36mpatched_init\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m                         \u001b[0merror\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                     )\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mobj_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpatched_init\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, env, logger_creator, **kwargs)\u001b[0m\n\u001b[1;32m    515\u001b[0m         }\n\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    518\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0mlogger_creator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogger_creator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, logger_creator, remote_checkpoint_dir, sync_config, storage)\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"StorageContext on the TRAINABLE:\\n{storage}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m         \u001b[0msetup_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msetup_time\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mSETUP_TIME_THRESHOLD\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\u001b[0m in \u001b[0;36msetup\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    637\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_init\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m             \u001b[0;31m# Create a set of env runner actors via a WorkerSet.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 639\u001b[0;31m             self.workers = WorkerSet(\n\u001b[0m\u001b[1;32m    640\u001b[0m                 \u001b[0menv_creator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_creator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m                 \u001b[0mvalidate_env\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_env\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, env_creator, validate_env, default_policy_class, config, num_workers, local_worker, logdir, _setup)\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_setup\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m                 self._setup(\n\u001b[0m\u001b[1;32m    158\u001b[0m                     \u001b[0mvalidate_env\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_env\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m                     \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py\u001b[0m in \u001b[0;36m_setup\u001b[0;34m(self, validate_env, config, num_workers, local_worker)\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0;31m# Create a local worker, if needed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlocal_worker\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m             self._local_worker = self._make_worker(\n\u001b[0m\u001b[1;32m    248\u001b[0m                 \u001b[0mcls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_runner_cls\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m                 \u001b[0menv_creator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_env_creator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py\u001b[0m in \u001b[0;36m_make_worker\u001b[0;34m(self, cls, env_creator, validate_env, worker_index, num_workers, recreated_worker, config, spaces)\u001b[0m\n\u001b[1;32m    923\u001b[0m         ] = None,\n\u001b[1;32m    924\u001b[0m     ) -> Union[RolloutWorker, ActorHandle]:\n\u001b[0;32m--> 925\u001b[0;31m         worker = cls(\n\u001b[0m\u001b[1;32m    926\u001b[0m             \u001b[0menv_creator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv_creator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m             \u001b[0mvalidate_env\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_env\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, env_creator, validate_env, config, worker_index, num_workers, recreated_worker, log_dir, spaces, default_policy_class, dataset_shards, tf_session_creator)\u001b[0m\n\u001b[1;32m    468\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspaces\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspaces\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_policy_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefault_policy_class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m         self.policy_dict, self.is_policy_to_train = self.config.get_multi_agent_setup(\n\u001b[0m\u001b[1;32m    471\u001b[0m             \u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mspaces\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspaces\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm_config.py\u001b[0m in \u001b[0;36mget_multi_agent_setup\u001b[0;34m(self, policies, env, spaces, default_policy_class)\u001b[0m\n\u001b[1;32m   2971\u001b[0m                     \u001b[0mobs_space\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2972\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2973\u001b[0;31m                     raise ValueError(\n\u001b[0m\u001b[1;32m   2974\u001b[0m                         \u001b[0;34m\"`observation_space` not provided in PolicySpec for \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2975\u001b[0m                         \u001b[0;34mf\"{pid} and env does not have an observation space OR \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: `observation_space` not provided in PolicySpec for always_same and env does not have an observation space OR no spaces received from other workers' env(s) OR no `observation_space` specified in config!"
     ]
    }
   ],
   "source": [
    "def select_policy(agent_id, episode, **kwargs):\n",
    "        if agent_id == \"attaccante\":\n",
    "            return \"learned\"\n",
    "        else:\n",
    "            return random.choice([\"always_same\", \"beat_last\"])\n",
    "\n",
    "config = (\n",
    "        PGConfig()\n",
    "        .environment(\"grouped_test\")\n",
    "        .framework(\"torch\")\n",
    "        .rollouts(\n",
    "            num_rollout_workers=0,\n",
    "            num_envs_per_worker=4,\n",
    "            rollout_fragment_length=10,\n",
    "        )\n",
    "        .training(\n",
    "            train_batch_size=200,\n",
    "            gamma=0.9,\n",
    "        )\n",
    "        .multi_agent(\n",
    "            policies={\n",
    "                \"always_same\": PolicySpec(policy_class=AlwaysSameHeuristic),\n",
    "                \"beat_last\": PolicySpec(policy_class=BeatLastHeuristic),\n",
    "                \"learned\": PolicySpec(\n",
    "                    config=AlgorithmConfig.overrides(\n",
    "                        model={\"use_lstm\": False},\n",
    "                        framework_str=\"torch\",\n",
    "                    )\n",
    "                ),\n",
    "            },\n",
    "            policy_mapping_fn=select_policy,\n",
    "            policies_to_train=[\"learned\"],\n",
    "        )\n",
    "        .reporting(metrics_num_episodes_for_smoothing=200)\n",
    "        # Use GPUs iff `RLLIB_NUM_GPUS` env var set to > 0.\n",
    "        .resources(num_gpus=int(os.environ.get(\"RLLIB_NUM_GPUS\", \"0\")))\n",
    "    )\n",
    "algo = config.build()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
